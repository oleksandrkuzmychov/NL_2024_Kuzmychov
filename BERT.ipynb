{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8253588,"sourceType":"datasetVersion","datasetId":4897679}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom torch.nn.functional import softmax\nimport transformers\nfrom transformers import AutoModel, BertTokenizerFast\n\n# ГПУ для більших швидких обчислень\ndevice = torch.device(\"cuda\")","metadata":{"execution":{"iopub.status.busy":"2024-09-10T10:18:01.240741Z","iopub.execute_input":"2024-09-10T10:18:01.242073Z","iopub.status.idle":"2024-09-10T10:18:01.247876Z","shell.execute_reply.started":"2024-09-10T10:18:01.242031Z","shell.execute_reply":"2024-09-10T10:18:01.246925Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv(\"../input/sarcasm-detection/sarcasm_detection.csv\")\ndata = data[['headline', 'is_sarcastic']]\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2024-09-10T10:18:01.250114Z","iopub.execute_input":"2024-09-10T10:18:01.250459Z","iopub.status.idle":"2024-09-10T10:18:01.405484Z","shell.execute_reply.started":"2024-09-10T10:18:01.250425Z","shell.execute_reply":"2024-09-10T10:18:01.404554Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"                                            headline  is_sarcastic\n0  thirtysomething scientists unveil doomsday clo...             1\n1  dem rep. totally nails why congress is falling...             0\n2  eat your veggies: 9 deliciously different recipes             0\n3  inclement weather prevents liar from getting t...             1\n4  mother comes pretty close to using word 'strea...             1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>headline</th>\n      <th>is_sarcastic</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>thirtysomething scientists unveil doomsday clo...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>dem rep. totally nails why congress is falling...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>eat your veggies: 9 deliciously different recipes</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>inclement weather prevents liar from getting t...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>mother comes pretty close to using word 'strea...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"data['is_sarcastic'].value_counts(normalize = True)","metadata":{"execution":{"iopub.status.busy":"2024-09-10T10:18:01.406627Z","iopub.execute_input":"2024-09-10T10:18:01.406914Z","iopub.status.idle":"2024-09-10T10:18:01.414940Z","shell.execute_reply.started":"2024-09-10T10:18:01.406890Z","shell.execute_reply":"2024-09-10T10:18:01.414078Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"is_sarcastic\n0    0.541679\n1    0.458321\nName: proportion, dtype: float64"},"metadata":{}}]},{"cell_type":"markdown","source":"Бачимо, що значного дізбалансу в даних нема. Тому можемо не добавляти ваги для класів","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import DataLoader, random_split, Dataset\nfrom torchvision import datasets\nfrom torchvision.transforms import ToTensor\nimport re\nbatch_size = 32\n\ntokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n\nclass SarcasmDataset(Dataset):\n    '''\n    Класс сімейства Датасет для наших даних \n    '''\n    def __init__(self, data, transform = None):\n        '''\n        Конструктор\n        '''\n        self.data = data\n        self.transform = transform\n        \n        \n    def __len__(self):\n        return len(self.data)\n\n    \n    def __getitem__(self, idx):\n\n        item = self.data.iloc[idx]\n        \n        target = int(item['is_sarcastic'])\n        sentence = item.headline\n        'Токенайзуємо речення та екстрактимо токени зі масками'\n        tokens = tokenizer.encode_plus(\n            sentence,\n            add_special_tokens = True,\n            max_length=64,\n            pad_to_max_length=True,\n            return_attention_mask=True,\n            return_tensors='pt'\n        )\n        \n\n        seq = tokens['input_ids']\n        mask = tokens['attention_mask']\n        \n        if self.transform:\n            seq = self.transform(seq)\n        \n        'Повертаємо токени зі масками речення, та класс, до якого належить оброблене речення'\n        return {\n            'input_ids': seq,\n            'attention_mask': mask,\n            'class': target\n         }\n    \ndataset = SarcasmDataset(data) #Ініцюалізуємо\ntrainset, validset = random_split(dataset, [0.7, 0.3]) #Ділимо на train та valid\n#Та ініцюалізуємо train та test-дані \ntrain_loader = DataLoader(trainset, shuffle=True, batch_size=batch_size)\nvalid_loader = DataLoader(validset, shuffle=True, batch_size=batch_size)","metadata":{"execution":{"iopub.status.busy":"2024-09-10T10:18:01.417052Z","iopub.execute_input":"2024-09-10T10:18:01.417507Z","iopub.status.idle":"2024-09-10T10:18:01.635691Z","shell.execute_reply.started":"2024-09-10T10:18:01.417477Z","shell.execute_reply":"2024-09-10T10:18:01.634721Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"class BERT_Arch(nn.Module):\n\n    def __init__(self):\n        super(BERT_Arch, self).__init__()\n        \n        self.bert = AutoModel.from_pretrained('bert-base-uncased')\n      \n        self.fc1 = nn.Sequential(nn.Dropout(0.1),\n                                nn.Linear(768,1))\n\n    #define the forward pass\n    def forward(self, sent_id, mask):\n        \n        #Берт Модель \n        _, cls_hs = self.bert(sent_id, attention_mask=mask, return_dict=False)\n        #І поверх неї модель класифікац\n\n        \n        # output layer\n        x = self.fc1(cls_hs)\n\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-09-10T10:18:01.636888Z","iopub.execute_input":"2024-09-10T10:18:01.637253Z","iopub.status.idle":"2024-09-10T10:18:01.643951Z","shell.execute_reply.started":"2024-09-10T10:18:01.637212Z","shell.execute_reply":"2024-09-10T10:18:01.642954Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AdamW\nimport tqdm\nfrom torchmetrics.classification import BinaryAccuracy\nmodel = BERT_Arch().to(dtype=torch.float16, device='cuda')\n# define the optimizer\nloss_f = torch.nn.BCEWithLogitsLoss()\noptimizer = AdamW(model.parameters(),lr = 1e-5) \nval_loss_g = [1]\nmetrics = BinaryAccuracy().to(device)\ni = 1;\n\nwhile True:\n    print(f\"Epoch {i}\")\n    # Train loop\n    train_tqdm = tqdm.tqdm(train_loader)\n    model.train()\n    for batch in train_tqdm:\n        input_ids, attention_mask, cls = batch['input_ids'], batch['attention_mask'], batch['class']\n\n        # Передаємо до ГПУ наші тензори'\n        input_ids, attention_mask, cls = input_ids.to(device), attention_mask.to(device), cls.to(device)\n        #Онуляємо градієнти \n        optimizer.zero_grad()\n        model.zero_grad()\n        \n        #Предиктимо класи\n        output = model(input_ids.squeeze(1), attention_mask.squeeze(1)).squeeze(1)\n        \n        # Рахуємо трати\n        L = loss_f(output, cls.float())\n        \n        acc = accuracy(softmax(output), cls)\n        #Передаємо до моделі градієнт втрат\n        L.backward()\n        \n        \n        optimizer.step() #Оновлення параметрів отпимізатору\n        train_tqdm.set_description(f\"Train loss: {L.item()}  Accuracy -- {acc}\") #Вивід у термінал значення втрат\n        train_tqdm.refresh()\n        \n    valid_tqdm = tqdm.tqdm(valid_loader)    \n    model.eval()\n    \n    with torch.no_grad():\n        val_loss = []\n        acc = []\n        for batch in valid_tqdm:\n            input_ids, attention_mask, cls = batch['input_ids'], batch['attention_mask'], batch['class']\n\n            input_ids, attention_mask, cls = input_ids.to(device), attention_mask.to(device), cls.to(device)\n            \n            output = model(input_ids.squeeze(1), attention_mask.squeeze(1)).squeeze(1)\n\n            L = loss_f(output, cls.float())\n            val_loss.append(L.item())\n            \n            acc.append(metrics(output, cls).cpu())\n            \n            \n            valid_tqdm.set_description(f\"Val loss: {L.item()}\")\n            valid_tqdm.refresh()\n            \n    val_loss_g.append(np.mean(val_loss))\n    \n    if abs(val_loss_g[-2] - val_loss_g[-1]) > 0.0001:\n        print(f\"Epoch {i}, Val loss: {np.mean(val_loss)}\")\n        print(f\"Epoch {i}, Accuracy: {np.mean(acc)}\")\n        i += 1\n    else:\n        break\n    ","metadata":{"execution":{"iopub.status.busy":"2024-09-10T10:28:35.758819Z","iopub.execute_input":"2024-09-10T10:28:35.759657Z","iopub.status.idle":"2024-09-10T10:41:49.461471Z","shell.execute_reply.started":"2024-09-10T10:28:35.759624Z","shell.execute_reply":"2024-09-10T10:41:49.460502Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Epoch 1\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/1211 [00:00<?, ?it/s]/tmp/ipykernel_34/2639436552.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  acc = accuracy(softmax(output), cls)\nTrain loss: 0.7043365836143494  Accuracy -- 0.4000000059604645: 100%|██████████| 1211/1211 [03:48<00:00,  5.29it/s]\nVal loss: 0.703502357006073: 100%|██████████| 519/519 [00:35<00:00, 14.63it/s] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 1, Val loss: 0.6896028604810638\nEpoch 1, Accuracy: 0.5413984060287476\nEpoch 2\n","output_type":"stream"},{"name":"stderr","text":"Train loss: 0.6468445062637329  Accuracy -- 0.699999988079071: 100%|██████████| 1211/1211 [03:49<00:00,  5.29it/s]\nVal loss: 0.7129405736923218: 100%|██████████| 519/519 [00:35<00:00, 14.63it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2, Val loss: 0.6898658112524102\nEpoch 2, Accuracy: 0.5413710474967957\nEpoch 3\n","output_type":"stream"},{"name":"stderr","text":"Train loss: 0.6604858636856079  Accuracy -- 0.4000000059604645: 100%|██████████| 1211/1211 [03:48<00:00,  5.29it/s]\nVal loss: 0.6830333471298218: 100%|██████████| 519/519 [00:35<00:00, 14.65it/s]\n","output_type":"stream"}]}]}